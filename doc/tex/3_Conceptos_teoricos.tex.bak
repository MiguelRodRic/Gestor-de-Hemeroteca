\capitulo{3}{Conceptos teóricos}
A continuación se explicarán los conceptos de carácter teórico más relevantes del proyecto, de modo que se puedan comprender con más exactitud aquellas partes con una mayor complejidad.


\section{Minería de Datos}

En los siguientes apartados se van a desarrollar todos los conceptos del proyecto relacionados con la minería de datos, esenciales en la parte de predicción del perfil ideológico de las noticias.

Inicialmente, se resumirá de forma breve el concepto de aprendizaje automático para a continuación hablar de la técnica del bag of words y de los clasificadores que se han usado en combinación con ésta.

\subsection{Machine learning}

El aprendizaje automático, o machine learning \cite{andrieu2003introduction}, es una técnica del campo de la Inteligencia Artificial que permite a un computador simular un proceso de aprendizaje \cite{wiki:machinelearning}. Las predicciones que se realizan en esta aplicación se basan en el machine learning. En términos generales, se pueden distinguir tres tipos de aprendizaje distinto en función de la supervisión del programador:

\begin{itemize}

\item Aprendizaje supervisado, donde se indica a qué clase pertenece cada miembro del conjunto de entrenamiento.

\item Aprendizaje no supervisado, donde los elementos del conjunto de entrenamiento no tienen asignadas clases y a lo largo del proceso se crean de forma automática 
agrupaciones entre los elementos similares.

\item Aprendizaje semisupervisado, donde hay miembros del conjunto que tienen asignada una clase y otros no, para mejorar la exactitud del aprendizaje \cite{wiki:semisupervised}.

\end{itemize}

Respecto al proyecto, el aprendizaje que se ha realizado es de tipo supervisado, ya que en el corpus de entrenamiento todas las noticias tienen clase asignada.

No obstante, podría decirse que conceptualmente tiene elementos del aprendizaje semisupervisado, pues pasado cierto tiempo, lo más probable es que parte del corpus del usuario tenga la clase asignada por una predicción realizada previamente, y no por un etiquetado manual, algo que es característico de este tipo de aprendizaje.

\subsection{Bag of Words}

Bag of Words \cite{bagofwords} \cite{wiki:bagofwords} es una técnica de agrupación de palabras relacionada con el Procesamiento de Lenguaje Natural. Consiste en, a partir de un texto de muestra, almacenar la frecuencia de aparición de cada palabra distinta que esté contenida en el texto, independientemente del orden de aparición, y sin tener en cuenta el contexto en el que aparezca. 

En términos de estructuras de datos, un bag of words se suele almacenar con un formato de diccionario, donde la clave está formada por cada palabra distinta, y el valor
es la frecuencia de aparición de esa palabra.

Vamos a ilustrar lo explicado con un ejemplo. A continuación, mostramos unas frases de ejemplo:

\begin{itemize}

\item ``No se puede fumar en establecimientos públicos"

\item ``Se puede fumar en residencias privadas"

\item ``En general, no se debería fumar"

\end{itemize}

Teniendo estas frases, si almacenamos la frecuencia de las palabras distintas para cada una de ellas, y las combinamos, tendríamos una tabla como la 3.1.

\tablaSmall{Frecuencias de palabras en las frases de ejemplo}{c c}{ejemplofrecuenciaspalabras}
{ \multicolumn{1}{c}{Palabras} & Frecuencia \\}{ 
No & 2\\
Se & 3\\
Puede & 2\\
Fumar & 3\\
En & 3\\
Establecimientos & 1\\
Públicos & 1\\
Residencias & 1\\
Privadas & 1\\
General & 1\\
Debería & 1\\
} 

El potencial de esta técnica para análisis de texto es muy interesante. Por ejemplo, y por relacionarlo con este proyecto, se podría asociar una etiqueta a cada una de las frases propuestas, como ``A favor de los fumadores'' y ``En contra de los fumadores''; para que una vez realizado el bag of words se pueda, entre otras funcionalidades, deducir las probabilidades que tiene cada una de las palabras del vocabulario obtenido de pertenecer a una de las dos clases.

De forma indirecta, lo previamente mencionado se puede usar para predecir cuál sería la ideología de una frase nueva, en función de las palabras de la misma que coinciden con el vocabulario ya almacenado por los clasificadores.

En el apartado ``Conceptos Avanzados de Bag of Words'' dentro de ``Aspectos Relevantes'' se explicarán con más detenimiento algunos aspectos más profundos de esta técnica.

\subsection{Clasificador Naive Bayes}

Este clasificador es muy popular en la clasificación de textos. Basa su funcionamiento en el Teorema de Bayes, que consiste en calcular la probabilidad de un evento dando relevancia al conocimiento previo de los factores relacionados consigo mismo \cite{bayestheorem}.

Llevando esto a la minería de datos, las probabilidades de que un elemento pertenezca a una clase van a ser potenciadas por el conocimiento previo del valor de cada uno de sus atributos. Además, se le añade la etiqueta de ``naive'' porque se considera que no hay ninguna relación de dependencia entre los diferentes atributos que componen ese elemento, aunque fuera del contexto de la clasificación estos atributos sí que estén fuertemente relacionados \cite{bayesclassifier}.

\subsection{Árbol de decisión}

Un árbol de decisión es una técnica para la toma de decisiones basado en la evaluación de atributos por separado. Cada árbol tiene un número de hojas, correspondiente al número de clases que se pueden devolver como resultado en función de la decisión que se haya tomado en la ``raíz'' del mismo \cite{decisiontree}.

\imagen{decisiontree1}{Ejemplo básico de funcionamiento de árbol de decisión}

Cuanto más complejo sea el árbol y mayor el número de nodos, se podrán evaluar decisiones más complejas y tener varios atributos en cuenta. Es una técnica usada con mucha frecuencia en el aprendizaje automático, ya que la combinación de muchos árboles de decisión forman lo que se conocen como ``bosques'', de los que hablaremos a continuación.

\subsection{Bagging}

También conocido como Bootstrap Agreggation \cite{breiman1996bagging}, es una técnica de aprendizaje supervisado, y más concretamente de clasificación, basada en la combinación de clasificadores. Su objetivo es el de reducir todo lo posible la varianza en la clasificación de datos. Consiste en obtener una predicción por votación a partir del promedio de los resultados obtenidos aplicando un gran número de clasificadores a distintas partes de los datos de entrenamiento.

De forma breve, podríamos explicar el proceso que realiza el Bagging en los siguientes pasos:

\begin{enumerate}

\item  A partir del conjunto de entrenamiento, se obtienen $m$ subconjuntos distintos, llamados ``bags'', que contienen combinaciones aleatorias de instancias del conjunto de entrenamiento.

\item Para cada ``bag'', se entrena un modelo con un clasificador distinto.

\item Todos los modelos resultantes se usan posteriormente para predecir el conjunto de prueba.

\item Para saber el resultado final, basta con saber cuál fue la clase que obtuvo más predicciones de todas las que se realizaron.

\end{enumerate}

Ésta técnica se usa mucho en los casos de predicciones basadas en árboles, como es el caso de los bosques aleatorios, que explicaremos a continuación. En la siguiente imagen se pueden observar de forma sencilla los pasos previamente mencionados.

\imagen{Bagging}{Esquema de funcionamiento básico de Bagging  \cite{baggingpicture}}


\subsection{Clasificador Random Forest}

El clasificador Random Forest es una técnica derivada del bagging aplicada a árboles de decisión \cite{randomforestclassifier}. Como su nombre sugiere, se basa en el uso de la combinación de varios árboles de clasificación. 

Cada uno de estos árboles se encarga de entrenar una instancia de los datos de entrenamiento, y la solución devuelta será la clase obtenida por votación de todos los árboles. 

El principal objetivo de realizar este proceso es el de, mediante los subconjuntos aleatorios, aumentar todo lo posible la precisión y controlar el sobreajuste de los resultados. Por ese motivo, si tenemos un mismo conjunto de entrenamiento para todas las pruebas, se obtendrá un menor margen de error en aquellas clasificaciones que tengan un mayor número de árboles, mientras que los resultados con un número mínimo de árboles no serán tan fiables \cite{randomforestclassifierberkeley}.

En la siguiente imagen se muestra un esquema de funcionamiento del bosque aleatorio, en la que se puede observar las similaridades con el proceso de Bagging explicado anteriormente.

\imagen{RandomForestExample}{Esquema de funcionamiento de bosque aleatorio \cite{randomforestpicture}}


\subsection{Vectorizador de texto}

Herramientas que convierten un documento de texto en matrices de tokens, que usualmente son palabras  \cite{countvectorizer}.
Son muy útiles para aplicar la técnica del bag of words a cualquier texto sin tener que recurrir a un conteo manual.

El tamaño de la matriz suele corresponderse con el tamaño del vocabulario obtenido a partir de la extracción de palabras del texto, aunque en la mayoría de herramientas el propio programador puede elegir un límite fijo para el tamaño. 

También es muy frecuente encontrar proyectos en los que se han usado vectorizadores para el tratamiento de imágenes, los cuales permiten transformarlas en formatos más flexibles para que puedan ser manejadas y exportadas con facilidad. No obstante, en contextos de investigación y análisis de textos, estas herramientas también son especialmente prácticas.



\section{Web Scraping}

El Web Scraping \cite{mitchell2015web} es una técnica, principalmente alternativa al uso de APIs de los propietarios de páginas web, para extraer información relevante de las propias páginas web.

De forma más concreta, lo que realiza un web scraper es la detección de información que se encuentra dentro de etiquetas concretas de un documento de tipo HTML, y su posterior extracción al indicarle las etiquetas en las que buscar. Es una técnica con un gran potencial, pues le evita al usuario la búsqueda manual de esta información, lo cual es una tarea especialmente ardua en los complejos códigos HTML de la mayoría de páginas web. 

A continuación mostramos un ejemplo sencillo de Web Scraping escrito en Python usando urllib2 y BeautifulSoup, en el que obtenemos el número de descargas de un repositorio en SourceForge:

\begin{enumerate}

\item Hacemos una petición http con la url indicada:

\imagen{WebScraping1}{Petición http}

\item Leemos el contenido de la url y se la pasamos al scraper:

\imagen{WebScraping2}{Lectura del contenido de la web}

\item A través del scraper, accedemos al texto de la etiqueta que queramos directamente:

\imagen{WebScraping3}{Acceso a la información deseada con el scraper}

\end{enumerate}



